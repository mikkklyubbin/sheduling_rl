{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27aa63b8-3820-48d4-8a95-b1fecce9ca75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8191d5-af5c-4d2e-bd11-313ade4297bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "print(\"done\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c714d2e-49ee-4dfb-8a17-991205596a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfcdc167-a7f0-4013-9eb7-837dd982ef82",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "HOMEDIR = os.getcwd()\n",
    "if \"jovyan\" in HOMEDIR:\n",
    "    HOMEDIR = \"/home/jovyan/sivtsov/graph_agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd9c9d0-2fe7-4d88-bb6a-bb80e5548427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crutch for code visibiltiy\n",
    "sys.path.append(HOMEDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c19a61-c7e9-444e-af54-f9b28b3d5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "STARPU_HOME = HOMEDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44efa43b-b9a9-4a55-b2dd-a80189617ee6",
   "metadata": {},
   "source": [
    "# Inspect\n",
    "\n",
    "here you can dump whole graph, to see how it looks like. It is traced from nntile model (which created from pytorch). So operations should be as in original model (there are tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51626044-e8bf-4273-9534-4efed4856d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from subprocess import Popen, PIPE\n",
    "from simulator.nnt_graph_sched_utils import (\n",
    "    BufferizedIterator, \n",
    "    StreamGraphLinesCatcher,\n",
    "    GraphParser,\n",
    "    SchedulerTaskGraph,\n",
    "    TimingsParser,\n",
    "    PrioritySerializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4229fd94-fbb8-4e5f-8cc1-95a4cb468910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_trainer import (\n",
    "    construct_program_cmd,\n",
    "    custom_dynamic_policy, \n",
    "    custom_random_policy,\n",
    "    GraphRunner\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f35d6c9f-8d00-453f-983f-01ed86e17f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run process:  python3 /workspace/graph_dataset.py --file /workspace/graph_tracer/traced_graph_2_small_layers.json --ncpus 1 --ncuda 0 --niters 1\n",
      "Process pid: 527\n"
     ]
    }
   ],
   "source": [
    "file = f\"{HOMEDIR}/graph_tracer/traced_graph_2_small_layers.json\"\n",
    "ncpus = 1\n",
    "ncuda = 0\n",
    "niters = 1\n",
    "manual_sampling = False\n",
    "\n",
    "verbose = True\n",
    "\n",
    "gr = GraphRunner(file, ncpus, ncuda, niters, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "gr.run(interactive=False, starpu_home=STARPU_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56a49370-5bfd-4dc6-aa18-b2f1fd6d7958",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = gr.skip_initialization_routine()\n",
    "graph_init = gr.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c2eb965-3ba6-45d0-b9aa-760d7b03fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://graphviz.org/\n",
    "gi_gv = graph_init.to_graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2fa96c-5bf6-4fd9-9120-7cdec6b71237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/small.dot.pdf'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gi_gv.render(f\"{HOMEDIR}/small.dot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e870b-b71d-4d53-b849-133901efd4f0",
   "metadata": {},
   "source": [
    "# Slice the graph\n",
    "\n",
    "For better convergence, debug and replay buffer purposes, you have the ability to provide slice indicies for graph\n",
    "\n",
    "Note, you should use interactive mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "282d3be1-379f-4c44-aa7b-a2be64a1c13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run process:  python3 /workspace/graph_dataset.py --file /workspace/graph_tracer/traced_graph_2_small_layers.json --ncpus 1 --ncuda 0 --niters 1 --manual_sampling\n",
      "Process pid: 536\n"
     ]
    }
   ],
   "source": [
    "file = f\"{HOMEDIR}/graph_tracer/traced_graph_2_small_layers.json\"\n",
    "ncpus = 1\n",
    "ncuda = 0\n",
    "niters = 1\n",
    "manual_sampling = True\n",
    "\n",
    "verbose = True\n",
    "\n",
    "gr = GraphRunner(file, ncpus, ncuda, niters, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "gr.run(interactive=False, starpu_home=STARPU_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4133c9a-6729-41a8-a52b-6f8c8ef33542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "operations = config[\"calls\"]\n",
    "total_operations = len(operations)\n",
    "total_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4de72f3c-3643-4692-9c3a-aab7ca48229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gr.skip_initialization_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adc67dc2-92ef-40c3-b903-16662fb18b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_slice = gr.next([50,60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f885f3-e9c7-4b45-90cb-ecf450127a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that where are actually also initialization ops \n",
    "len(graph_slice.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221e3c5-eb11-4792-bcd1-c7b95c0e76f1",
   "metadata": {},
   "source": [
    "# Collect statistics\n",
    "\n",
    "First, you need to simulate graph multiple times to collect starpu statistics (nntile based on starpu)   \n",
    "It will give you timings for every operation in graph\n",
    "\n",
    "Note, they are statistically collected and will not be updated during next simulations runs. So you can't train on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "471e44f2-5cce-4361-a65b-b88d985280eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = f\"{HOMEDIR}/graph_tracer/traced_graph_2_small_layers.json\"\n",
    "# ncpus = 4\n",
    "# ncuda = 0\n",
    "# niters = 25\n",
    "# manual_sampling = False\n",
    "\n",
    "# verbose = True\n",
    "\n",
    "# gr = GraphRunner(file, ncpus, ncuda, niters, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "# gr.run(interactive=False, collect_metrics=True)\n",
    "# gr.p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da11078-2c57-4625-b792-47100823c361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = f\"{HOMEDIR}/graph_tracer/traced_graph_8_layers.json\"\n",
    "# ncpus = 4\n",
    "# ncuda = 1\n",
    "# niters = 25\n",
    "# manual_sampling = False\n",
    "\n",
    "# verbose = True\n",
    "\n",
    "# gr = GraphRunner(file, ncpus, ncuda, niters, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "# gr.run(interactive=False, collect_metrics=True, starpu_home=STARPU_HOME)\n",
    "# gr.p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16d35aed-b0e2-40a5-9e90-534c6d6d74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = f\"{HOMEDIR}/graph_tracer/traced_graph_4_layers.json\"\n",
    "# ncpus = 4\n",
    "# ncuda = 0\n",
    "# niters = 25\n",
    "# manual_sampling = False\n",
    "\n",
    "# verbose = True\n",
    "\n",
    "# gr = GraphRunner(file, ncpus, ncuda, niters, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "# gr.run(interactive=False, collect_metrics=True, starpu_home=STARPU_HOME)\n",
    "# gr.p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5054babc-bc02-4a60-a421-5a5f88cc01c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = f\"{HOMEDIR}/graph_tracer/traced_graph_16_layers.json\"\n",
    "# ncpus = 4\n",
    "# ncuda = 0\n",
    "# niters = 25\n",
    "# manual_sampling = False\n",
    "\n",
    "# verbose = True\n",
    "\n",
    "# gr = GraphRunner(file, ncpus, ncuda, niters, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "# gr.run(interactive=False, collect_metrics=True, starpu_home=STARPU_HOME)\n",
    "# gr.p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73248bb2-27cc-4077-80de-1200d8dbab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # But if all iterations completed - should be fine\n",
    "# gr.stderr_iter.buffer[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ac0cb-0651-4b01-a9ac-153a41b695f8",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "here I suggest you to decouple some logic   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1377506-fb81-463e-9ed0-b3a7375eb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_gr_width:int = 512\n",
    "num_device:int  = ncpus + ncuda\n",
    "device_metr:int = 1\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 512)\n",
    "        self.layer4 = nn.Linear(512, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.layer4(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36fb34d8-b501-48b4-94e5-f66e395e9ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run process:  python3 /workspace/graph_dataset.py --file /workspace/graph_tracer/traced_graph_16_layers.json --ncpus 1 --ncuda 0 --niters 1 --manual_sampling\n",
      "Process pid: 542\n",
      "<graph_trainer.GraphRunner object at 0x7f5514e20370>\n",
      "n_nodes: 1055 serialized: 1\n",
      "Wait timings\n",
      "Compute timings: t_begin=4319484436044, t_end=4325244326644, delta_ms=5759.8906\n"
     ]
    }
   ],
   "source": [
    "def initialize():\n",
    "    name_op_to_id = {}\n",
    "    gr = GraphRunner(f\"{HOMEDIR}/graph_tracer/traced_graph_16_layers.json\", ncpus, ncuda, 1, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "    gr.run()\n",
    "    print(gr)\n",
    "    _ = gr.skip_initialization_routine()\n",
    "    graph = gr.next()\n",
    "    for i, (_, n) in enumerate(graph.nodes.items()):\n",
    "      if n.name != 'NULL':\n",
    "        name_op_to_id[n.name] = -1\n",
    "    id:int = 0\n",
    "    for i, (_, n) in enumerate(graph.nodes.items()):\n",
    "        if n.name != 'NULL':\n",
    "            if (name_op_to_id[n.name] == -1):\n",
    "                name_op_to_id[n.name] = id\n",
    "                id += 1\n",
    "    gr.serialize_priorities(graph)\n",
    "    timings = gr.wait_get_timings()\n",
    "    return [id, name_op_to_id]\n",
    "[col_ops, name_op_to_id] = initialize()\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.001\n",
    "EPS_END = 0.00001\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "NODE_METRIKS = 4\n",
    "len_operations_data:int = col_ops + NODE_METRIKS\n",
    "SIZE_BACKET = 30\n",
    "MEMORY_SIZE = 10000\n",
    "EPOCH_BEFORE_BEST_REC = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3074362e-b6d3-405b-853a-99ebfd55a333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_slice:\n",
    "  free_nodes = []\n",
    "  cnt_in = {}\n",
    "  deeps = {}\n",
    "  gr = []\n",
    "  fr_t = []\n",
    "  allgr = []\n",
    "  cur_prio = int(1e9)\n",
    "  load_of_dev = []\n",
    "  def __init__(self, gr) -> None:\n",
    "      global num_device\n",
    "      graph = gr.next()\n",
    "      self.free_nodes = []\n",
    "      self.cnt_in = {}\n",
    "      self.load_of_dev = [0] * num_device\n",
    "      self.sum_time = 0\n",
    "      self.sum_dim = 0\n",
    "      \n",
    "      print(len(graph.nodes.items()))\n",
    "      for i, (_, n) in enumerate(graph.nodes.items()):\n",
    "        if n.name != 'NULL':\n",
    "            self.cnt_in[n.addr] = len(n.inc)\n",
    "            if (len(n.inc) == 0):\n",
    "              self.free_nodes.append(n.addr)\n",
    "      print(\"graph_gened\")\n",
    "      self.cur_prio = int(1e9)\n",
    "      self.gr = graph.nodes\n",
    "      for el in self.free_nodes:\n",
    "          self.deeps[el] = 0\n",
    "      self.fr_t = gr\n",
    "      self.allgr = graph\n",
    "      self.last_measure = 0\n",
    "      self.dlt = 0\n",
    "      for i, (_, n) in enumerate(graph.nodes.items()):\n",
    "        if n.name != 'NULL':\n",
    "            self.sum_time += self.gr[n.addr].perfmodels[0].length_mcs\n",
    "  def make_decision(self, v:int, mask_dev) -> bool:\n",
    "      if (v >= len(self.free_nodes)):\n",
    "          return False\n",
    "      v = self.free_nodes[v]\n",
    "      self.gr[v].prio = self.cur_prio\n",
    "      self.gr[v].device_affinity = mask_dev\n",
    "      self.cur_prio-=1\n",
    "      \n",
    "      self.free_nodes.remove(v)\n",
    "      prob_used = []\n",
    "      for  i in range(num_device):\n",
    "          prob_used.append(i)\n",
    "      for el in prob_used:\n",
    "          self.load_of_dev[el] += (self.gr[v].perfmodels[0].length_mcs / self.sum_time / len(prob_used))\n",
    "      for el in  self.gr[v].out:\n",
    "        if (self.gr[el].name != 'NULL'):\n",
    "            self.cnt_in[el]-=1\n",
    "            if (self.cnt_in[el] == 0):\n",
    "              self.deeps[el] = 0\n",
    "              for u in self.gr[el].inc:\n",
    "                  self.deeps[el] = max(self.deeps[el], self.deeps[v] + 1)\n",
    "              self.free_nodes.append(el)\n",
    "      return True\n",
    "\n",
    "  def get_el_data(self, v:int, l:int, res):\n",
    "    global name_op_to_id, col_ops\n",
    "    res[l + name_op_to_id[self.gr[v].name]] = 1\n",
    "    res[l + col_ops] = self.deeps[v] / len(self.gr)\n",
    "    res[l + col_ops + 1] = len(self.gr[v].out) / len(self.gr)\n",
    "    res[l + col_ops + 2] = self.gr[v].perfmodels[0].length_mcs/ self.sum_time\n",
    "  def get_dev_data(self, j:int, l:int, res):\n",
    "    res[l] = self.load_of_dev[j] \n",
    "  def get_state_vector(self):\n",
    "    global max_gr_width, len_operations_data, col_ops\n",
    "    res = np.zeros(max_gr_width * len_operations_data + num_device * device_metr)\n",
    "    l = 0;\n",
    "    for el in self.free_nodes:\n",
    "      self.get_el_data(el, l, res)\n",
    "      l += len_operations_data\n",
    "    for j in range(num_device):\n",
    "      self.get_dev_data(j, l, res)\n",
    "      l += device_metr\n",
    "    return res\n",
    "  def get_timings(self):\n",
    "    while (len(self.free_nodes)> 0):\n",
    "        self.make_decision(0, 0)\n",
    "    self.allgr.nodes = self.gr\n",
    "    self.fr_t.serialize_priorities(self.allgr)\n",
    "    cur_t = self.fr_t.wait_get_timings()\n",
    "    print(self.last_measure)\n",
    "    print(cur_t)\n",
    "    self.dlt = self.last_measure - cur_t\n",
    "    self.last_measure = cur_t\n",
    "    return cur_t\n",
    "  def get_time(self, v:int):\n",
    "    return (-self.last_measure * self.gr[v].perfmodels[0].length_mcs/ self.sum_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf920ca6-941e-42d3-9bcb-c09d1c500f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba1605b1-49d5-418f-ad21-88549b683ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair(sz:int):\n",
    "    res = np.random.randint(0, sz, 2)\n",
    "    if (res[0] > res[1]):\n",
    "        swap(res[0], res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5801b6d9-9730-49ff-b352-9fefa0ec2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineAgent:\n",
    "    def __init__(self, strategy=\"random\", len_operations_data = 0, max_gr_width = 0, num_device = 0, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        here you can store actual model\n",
    "        and history of this agent (maybe resent, not whole)\n",
    "        \"\"\"\n",
    "        # self.agent_decisions = []\n",
    "        self.strategy = strategy\n",
    "        self.device = device\n",
    "        self.policy_net = DQN(len_operations_data * max_gr_width + num_device * device_metr, max_gr_width * num_device).to(device)\n",
    "        self.target_net = DQN(len_operations_data * max_gr_width + num_device * device_metr, max_gr_width * num_device).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.memory = ReplayMemory(MEMORY_SIZE)\n",
    "        self.steps_done = 0\n",
    "        self.num_device = num_device\n",
    "    def select_action(self, state, width:int = max_gr_width):\n",
    "        sample = random.random()\n",
    "        global EPS_END, EPS_START, EPS_DECAY\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "                res = self.policy_net(state).max(1).indices.view(1, 1)\n",
    "                return res\n",
    "        else:\n",
    "        #print(\"rd\")\n",
    "            return torch.tensor([[random.randint(0, width * num_device - 1)]])\n",
    "    def get_values(self, action):\n",
    "        res:int = action[0][0]\n",
    "        msk = [False] * self.num_device\n",
    "        msk[res % self.num_device] = 1\n",
    "        return [res // self.num_device, msk]\n",
    "\n",
    "    def schedule(self, graph):\n",
    "        \"\"\"\n",
    "        Here you just schedule train\n",
    "        Don't do expensive grapdient/state computations, just simulation step\n",
    "        \"\"\"\n",
    "        # custom_dynamic_policy/custom_random_policy. Look at graph_trainer.py\n",
    "        # First baseline you should beat is custom_random_policy. Because even simplest DFS is better\n",
    "        # Second - custom_dynamic_policy - DFS like approach. \n",
    "        # Probably you will just learn this strategy with your agent for now. \n",
    "        # To beat it, graph should be more complex, not strategy\n",
    "        if self.strategy==\"dynamic\":\n",
    "            custom_dynamic_policy(i, graph, strategy=self.device)\n",
    "            return\n",
    "        elif self.strategy==\"random\":\n",
    "            custom_random_policy(i, graph, strategy=self.device)\n",
    "            return\n",
    "        elif self.strategy == \"neural\":\n",
    "            pass\n",
    "        else:\n",
    "            return\n",
    "        my_slice = graph\n",
    "        state = torch.tensor(my_slice.get_state_vector(), dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        self.frc = []\n",
    "        good_steps:int = 0\n",
    "        steps = 0\n",
    "        pred = []\n",
    "        for t in count():\n",
    "            # print(\"trouble\", my_slice.free_nodes, pred)\n",
    "            assert my_slice.free_nodes != pred, \"fffffff\"\n",
    "            pred = my_slice.free_nodes.copy()\n",
    "            if (len(my_slice.free_nodes) == 0):\n",
    "                break;\n",
    "            steps += 1\n",
    "            action = self.select_action(state=state, width = len(my_slice.free_nodes))\n",
    "            [v, msk] = self.get_values(action)\n",
    "            #msk = [1,1,1,1]\n",
    "            #print(v, msk)\n",
    "            tm = -1\n",
    "            if (len(my_slice.free_nodes) > v):\n",
    "                tm = my_slice.free_nodes[v]\n",
    "            good:bool = my_slice.make_decision(v, msk)\n",
    "            #print(good)\n",
    "            nxt_state = torch.tensor(np.zeros(len_operations_data * max_gr_width + num_device * device_metr), dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            if (good):\n",
    "                good_steps += 1\n",
    "                nxt_state = torch.tensor(my_slice.get_state_vector(), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                self.frc.append([state, action, nxt_state, tm])\n",
    "            else:\n",
    "                self.memory.push(state, action, nxt_state, torch.tensor(-1, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "                action = torch.tensor([[random.randint(0, len(my_slice.free_nodes) * num_device - 1)]])\n",
    "                [v, msk] = self.get_values(action)\n",
    "                msk = [1,1,1,1]\n",
    "                tmp = my_slice.make_decision(v, msk)\n",
    "                assert tmp, \"random op should be good\"\n",
    "                nxt_state = torch.tensor(my_slice.get_state_vector(), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            state = nxt_state\n",
    "        print(\"good=\", good_steps / steps)\n",
    "        return good_steps / steps\n",
    "#\n",
    " #       # Perform one step of the optimization (on the policy network)\n",
    "#\n",
    " #       # Soft update of the target network's weights\n",
    "  #      # θ′ ← τ θ + (1 −τ )θ′\n",
    "\n",
    "class AgentTrainer:\n",
    "    def __init__(self,agent,  BATCH_SIZE = 128):\n",
    "        # self.optimizer = optimizer\n",
    "        # self.replay_buffer = replay_buffer\n",
    "        self.BATCH_SIZE = BATCH_SIZE\n",
    "        self.optimizer = optim.Adam(agent.policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "    \n",
    "    def step(self,agent, timing, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Called after getting timings from your agent decision\n",
    "        Here you can maintain optimizer states, compute grads, backpropagate and so on\n",
    "        Probably - here can be stored state for RL algos like replay buffer\n",
    "        \"\"\"\n",
    "        # Called after \n",
    "        if (len(agent.memory) < self.BATCH_SIZE):\n",
    "            return\n",
    "        transitions = agent.memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        state_action_values = agent.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = agent.target_net(non_final_next_states).max(1).values\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(agent.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "        global TAU\n",
    "        agent.target_net_state_dict = agent.target_net.state_dict()\n",
    "        agent.policy_net_state_dict = agent.policy_net.state_dict()\n",
    "        for key in agent.policy_net_state_dict:\n",
    "            agent.target_net_state_dict[key] = agent.policy_net_state_dict[key]*TAU + agent.target_net_state_dict[key]*(1-TAU)\n",
    "        agent.target_net.load_state_dict(agent.target_net_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427bc8b-c324-4705-9e1e-0cf2ab155a04",
   "metadata": {},
   "source": [
    "# Runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a96e623-1a7d-430f-b7cf-bb0eaada257f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 15:30:50.684957: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from graph_trainer import (\n",
    "    construct_program_cmd,\n",
    "    custom_dynamic_policy, \n",
    "    custom_random_policy,\n",
    "    GraphRunner\n",
    ")\n",
    "\n",
    "from interpretator import (\n",
    "    get_subset_inds\n",
    ")\n",
    "\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b53bd297-da19-466d-90c7-7255be1704cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graph_trainer import (\n",
    "#     construct_program_cmd,\n",
    "#     custom_dynamic_policy, \n",
    "#     custom_random_policy,\n",
    "#     GraphRunner\n",
    "# )\n",
    "\n",
    "# from interpretator import (\n",
    "#     get_subset_inds\n",
    "# )\n",
    "\n",
    "# import json\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import sys\n",
    "# import math\n",
    "# import random\n",
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import namedtuple, deque\n",
    "# from itertools import count\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# device = torch.device(\n",
    "#     \"cuda\" if torch.cuda.is_available() else\n",
    "#     \"mps\" if torch.backends.mps.is_available() else\n",
    "#     \"cpu\"\n",
    "# )\n",
    "# print(\"done\")\n",
    "# print(device)\n",
    "# HOMEDIR = os.getcwd()\n",
    "# if \"jovyan\" in HOMEDIR:\n",
    "#     HOMEDIR = \"/home/jovyan/sivtsov/graph_agent\"\n",
    "# sys.path.append(HOMEDIR)\n",
    "# STARPU_HOME = HOMEDIR\n",
    "# ncpus = 4\n",
    "# ncuda = 0\n",
    "# manual_sampling = False\n",
    "\n",
    "# verbose = True\n",
    "# file = f\"{HOMEDIR}/graph_tracer/traced_graph_2_small_layers.json\"\n",
    "# gr = GraphRunner(file, ncpus, ncuda, 100, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "# gr.run(interactive=True, starpu_home=STARPU_HOME)\n",
    "# _ = gr.skip_initialization_routine()\n",
    "# for i in range(gr.niters):\n",
    "#     # inds = get_subset_inds(rng, total_operations)\n",
    "#     inds = \"all\" # just use this if you want whole graph\n",
    "#     print(f\"{i=} {inds=}\")\n",
    "#     graph = gr.next(inds=inds)\n",
    "#     custom_dynamic_policy(i, graph, strategy=\"cpu\")\n",
    "#     # print(\"Nodes: \", len(graph.nodes))\n",
    "\n",
    "#     gr.serialize_priorities(graph)\n",
    "#     timings = gr.wait_get_timings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0dcf96c2-e819-4e45-9b64-8e898ba35684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_etalon(file, gr):\n",
    "    agent = BaselineAgent(strategy=\"dynamic\", device=\"cpu\" if ncuda==0 else \"cuda\")\n",
    "    trainer = AgentTrainer(agent)\n",
    "    timings_dynamic = []\n",
    "\n",
    "    for i in range(5):\n",
    "        # inds = get_subset_inds(rng, total_operations)\n",
    "        inds = \"all\" # just use this if you want whole graph\n",
    "        print(f\"{i=} {inds=}\")\n",
    "        graph = gr.next()\n",
    "\n",
    "        # print(\"Nodes: \", len(graph.nodes))\n",
    "\n",
    "        agent.schedule(graph)\n",
    "\n",
    "        gr.serialize_priorities(graph)\n",
    "        timings = gr.wait_get_timings()\n",
    "\n",
    "        trainer.step(agent, timings)\n",
    "    \n",
    "        timings_dynamic.append(timings)\n",
    "    return timings_dynamic[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27766eda-28dc-4294-9f8d-0c2e0cfb2526",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"{HOMEDIR}/graph_tracer/traced_graph_2_small_layers.json\"\n",
    "ncpus = 4\n",
    "ncuda = 0\n",
    "niters = 10\n",
    "manual_sampling = True\n",
    "\n",
    "verbose = True\n",
    "# verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2fafa599-683f-4f6e-8015-d117e458647a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "operations = config[\"calls\"]\n",
    "total_operations = len(operations)\n",
    "total_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ffc5f1e-0274-4af4-9fd4-aa917c0e9235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "agent = BaselineAgent(strategy=\"dynamic\", device=\"cpu\" if ncuda==0 else \"cuda\")\n",
    "trainer = AgentTrainer(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78cb86d8-4d74-4685-a783-e1743f824de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run process:  python3 /workspace/graph_dataset.py --file /workspace/graph_tracer/traced_graph_2_small_layers.json --ncpus 4 --ncuda 0 --niters 10 --manual_sampling\n",
      "Process pid: 551\n"
     ]
    }
   ],
   "source": [
    "gr = GraphRunner(file, ncpus, ncuda, niters, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "gr.run(interactive=True, starpu_home=STARPU_HOME)\n",
    "_ = gr.skip_initialization_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96ce2e27-2a20-4c8c-9ca9-86e86f0517a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d598c8ff-4125-4d83-9ec0-6aa8bdf5b66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4333847946817, t_end=4333958535948, delta_ms=110.589131\n",
      "i=1 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4333964803205, t_end=4334075891149, delta_ms=111.087944\n",
      "i=2 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4334081066503, t_end=4334231199524, delta_ms=150.133021\n",
      "i=3 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4334238517947, t_end=4334372113185, delta_ms=133.595238\n",
      "i=4 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4334377746055, t_end=4334504204299, delta_ms=126.458244\n",
      "i=5 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4334509986978, t_end=4334641580521, delta_ms=131.593543\n",
      "i=6 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4334647664975, t_end=4334779216579, delta_ms=131.551604\n",
      "i=7 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4334785183444, t_end=4334913860760, delta_ms=128.677316\n",
      "i=8 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4334920296441, t_end=4335045172875, delta_ms=124.876434\n",
      "i=9 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4335051696882, t_end=4335192596357, delta_ms=140.899475\n"
     ]
    }
   ],
   "source": [
    "timings_dynamic = []\n",
    "\n",
    "for i in range(gr.niters):\n",
    "    # inds = get_subset_inds(rng, total_operations)\n",
    "    inds = \"all\" # just use this if you want whole graph\n",
    "    print(f\"{i=} {inds=}\")\n",
    "    graph = gr.next(inds=inds)\n",
    "\n",
    "    # print(\"Nodes: \", len(graph.nodes))\n",
    "\n",
    "    agent.schedule(graph)\n",
    "\n",
    "    gr.serialize_priorities(graph)\n",
    "    timings = gr.wait_get_timings()\n",
    "\n",
    "    trainer.step(agent, timings)\n",
    "    \n",
    "    timings_dynamic.append(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309cd982-423e-4fff-b2c9-108db1cf4cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87010b-226d-443a-9ff4-37db0d314c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e1b2f8fd-469d-497c-bdaa-e7b38c34636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "agent = BaselineAgent(strategy=\"random\", device=\"cpu\" if ncuda==0 else \"cuda\")\n",
    "trainer = AgentTrainer(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fab1a0f4-8969-476b-a33e-3259aa3fbfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run process:  python3 /workspace/graph_dataset.py --file /workspace/graph_tracer/traced_graph_2_small_layers.json --ncpus 4 --ncuda 0 --niters 10 --manual_sampling\n",
      "Process pid: 559\n"
     ]
    }
   ],
   "source": [
    "gr = GraphRunner(file, ncpus, ncuda, niters, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "gr.run(interactive=True, starpu_home=STARPU_HOME)\n",
    "_ = gr.skip_initialization_routine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40fd9e9a-3a95-468a-8244-f9dd29ff779e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4340827845953, t_end=4340940344848, delta_ms=112.498895\n",
      "i=1 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4340946042048, t_end=4341056231952, delta_ms=110.189904\n",
      "i=2 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4341061401315, t_end=4341174063845, delta_ms=112.66253\n",
      "i=3 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4341179503594, t_end=4341288661046, delta_ms=109.157452\n",
      "i=4 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4341293960422, t_end=4341404591512, delta_ms=110.63109\n",
      "i=5 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4341410024968, t_end=4341521097284, delta_ms=111.072316\n",
      "i=6 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4341526443206, t_end=4341637783152, delta_ms=111.339946\n",
      "i=7 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4341643640773, t_end=4341755063965, delta_ms=111.423192\n",
      "i=8 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4341760750184, t_end=4341872020941, delta_ms=111.270757\n",
      "i=9 inds='all'\n",
      "n_nodes: 145 serialized: 144\n",
      "Wait timings\n",
      "Compute timings: t_begin=4341877476759, t_end=4341987914687, delta_ms=110.437928\n"
     ]
    }
   ],
   "source": [
    "timings_random = []\n",
    "\n",
    "for i in range(gr.niters):\n",
    "    # inds = get_subset_inds(rng, total_operations)\n",
    "    inds = \"all\" # just use this if you want whole graph\n",
    "    print(f\"{i=} {inds=}\")\n",
    "    graph = gr.next(inds=inds)\n",
    "\n",
    "    # print(\"Nodes: \", len(graph.nodes))\n",
    "\n",
    "    agent.schedule(graph)\n",
    "\n",
    "    gr.serialize_priorities(graph)\n",
    "    timings = gr.wait_get_timings()\n",
    "\n",
    "    trainer.step(agent, timings)\n",
    "    \n",
    "    timings_random.append(timings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ec83729-5311-4a73-8ed9-dd6e98f2a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len_operations_data)\n",
    "agent = BaselineAgent(\"neural\",len_operations_data,max_gr_width, num_device,  device=\"cpu\" if ncuda==0 else \"cuda\")\n",
    "trainer = AgentTrainer(agent)\n",
    "best_model = None\n",
    "best_res = 1e14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf614b08-1e3b-4092-b9bb-654725b6c2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_episodes = 300\n",
    "\n",
    "# for_gen = GraphRunner(file, ncpus, ncuda, num_episodes +((num_episodes + 19)//20) * 5, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "# for_gen.run(starpu_home=STARPU_HOME)\n",
    "# _ = for_gen.skip_initialization_routine()\n",
    "# BEST_TIME = 0\n",
    "# # BEST_TIME = get_etalon(file, gr)\n",
    "# for i_episode in range(num_episodes):\n",
    "#     if (i_episode % 20 == 0):\n",
    "#         BEST_TIME = get_etalon(file, for_gen)\n",
    "#     # Initialize the environment and get its state\n",
    "#     my_slice = graph_slice(for_gen)\n",
    "#     agent.schedule(my_slice)\n",
    "#     cur_t = my_slice.get_timings()\n",
    "    \n",
    "#     for el in agent.frc:\n",
    "#         reward = torch.tensor(my_slice.get_time(el[3]) / (BEST_TIME), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "#         agent.memory.push(el[0], el[1], el[2], reward)\n",
    "#     trainer.step(agent = agent, timing = cur_t)\n",
    "#     #if (COUNT_IT == INFO_SZ):\n",
    "#       #  print(\"good_st = \", good_steps)\n",
    "#       #  print(\"normal metric = \", get_timings_policy(5, for_gen) / get_timings_for_acc(5, for_gen))\n",
    "#       #  COUNT_IT = 0\n",
    "#       #  INFO_MEAN = 0\n",
    "#  #       if done:\n",
    "#   #          episode_durations.append(t + 1)\n",
    "#    #         plot_durations()\n",
    "#     #        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "92678d83-0de8-45b5-8be6-9b39c2df67f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run process:  python3 /workspace/graph_dataset.py --file /workspace/graph_tracer/traced_graph_32_layers.json --ncpus 4 --ncuda 0 --niters 190 --manual_sampling\n",
      "Process pid: 568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0 inds='all'\n",
      "n_nodes: 2095 serialized: 2094\n",
      "Wait timings\n",
      "Compute timings: t_begin=4362678443422, t_end=4374421198715, delta_ms=11742.755293\n",
      "i=1 inds='all'\n",
      "n_nodes: 2095 serialized: 2094\n",
      "Wait timings\n",
      "Compute timings: t_begin=4374477532540, t_end=4386190342322, delta_ms=11712.809782\n",
      "i=2 inds='all'\n",
      "n_nodes: 2095 serialized: 2094\n",
      "Wait timings\n",
      "Compute timings: t_begin=4386249895902, t_end=4397983888439, delta_ms=11733.992537\n",
      "i=3 inds='all'\n",
      "n_nodes: 2095 serialized: 2094\n",
      "Wait timings\n",
      "Compute timings: t_begin=4398039639003, t_end=4409713637726, delta_ms=11673.998723\n",
      "i=4 inds='all'\n",
      "n_nodes: 2095 serialized: 2094\n",
      "Wait timings\n",
      "Compute timings: t_begin=4409769440247, t_end=4421491437080, delta_ms=11721.996833\n",
      "2095\n",
      "graph_gened\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12289 is out of bounds for axis 0 with size 12289",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize the environment and get its state\u001b[39;00m\n\u001b[1;32m     16\u001b[0m my_slice \u001b[38;5;241m=\u001b[39m graph_slice(for_gen)\n\u001b[0;32m---> 17\u001b[0m ac \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_slice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m cur_t \u001b[38;5;241m=\u001b[39m my_slice\u001b[38;5;241m.\u001b[39mget_timings()\n\u001b[1;32m     19\u001b[0m epi_to_score\u001b[38;5;241m.\u001b[39mappend(cur_t \u001b[38;5;241m/\u001b[39m BEST_TIME)\n",
      "Cell \u001b[0;32mIn[28], line 59\u001b[0m, in \u001b[0;36mBaselineAgent.schedule\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     58\u001b[0m my_slice \u001b[38;5;241m=\u001b[39m graph\n\u001b[0;32m---> 59\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mmy_slice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m good_steps:\u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[25], line 74\u001b[0m, in \u001b[0;36mgraph_slice.get_state_vector\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m l \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_nodes:\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_el_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m   l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m len_operations_data\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_device):\n",
      "Cell \u001b[0;32mIn[25], line 63\u001b[0m, in \u001b[0;36mgraph_slice.get_el_data\u001b[0;34m(self, v, l, res)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_el_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, v:\u001b[38;5;28mint\u001b[39m, l:\u001b[38;5;28mint\u001b[39m, res):\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mglobal\u001b[39;00m name_op_to_id, col_ops\n\u001b[0;32m---> 63\u001b[0m   \u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname_op_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m   res[l \u001b[38;5;241m+\u001b[39m col_ops] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeeps[v] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgr)\n\u001b[1;32m     65\u001b[0m   res[l \u001b[38;5;241m+\u001b[39m col_ops \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgr[v]\u001b[38;5;241m.\u001b[39mout) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgr)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 12289 is out of bounds for axis 0 with size 12289"
     ]
    }
   ],
   "source": [
    "file = f\"{HOMEDIR}/graph_tracer/traced_graph_32_layers.json\"\n",
    "num_episodes = 150\n",
    "for_gen = GraphRunner(file, ncpus, ncuda, num_episodes+((num_episodes + EPOCH_BEFORE_BEST_REC - 1)//EPOCH_BEFORE_BEST_REC) * 5, True, verbose=verbose, home_dir=HOMEDIR)\n",
    "for_gen.run()\n",
    "_ = for_gen.skip_initialization_routine()\n",
    "\n",
    "BEST_TIME = 0\n",
    "TRAIN_FOR_TRACE = 20\n",
    "\n",
    "epi_to_ac = []\n",
    "epi_to_score = []\n",
    "for i_episode in range(num_episodes):\n",
    "    if (i_episode % EPOCH_BEFORE_BEST_REC == 0):\n",
    "        BEST_TIME = get_etalon(file, for_gen)\n",
    "    # Initialize the environment and get its state\n",
    "    my_slice = graph_slice(for_gen)\n",
    "    ac = agent.schedule(my_slice)\n",
    "    cur_t = my_slice.get_timings()\n",
    "    epi_to_score.append(cur_t / BEST_TIME)\n",
    "    epi_to_ac.append(ac)\n",
    "    if (cur_t / BEST_TIME < best_res):\n",
    "        best_res = cur_t /BEST_TIME\n",
    "        best_model = agent\n",
    "    for el in agent.frc:\n",
    "        reward = torch.tensor(my_slice.get_time(el[3]) / (BEST_TIME), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        agent.memory.push(el[0], el[1], el[2], reward)\n",
    "    for  i in range(TRAIN_FOR_TRACE):\n",
    "        trainer.step(agent = agent, timing = cur_t)\n",
    "    #if (COUNT_IT == INFO_SZ):\n",
    "      #  print(\"good_st = \", good_steps)\n",
    "      #  print(\"normal metric = \", get_timings_policy(5, for_gen) / get_timings_for_acc(5, for_gen))\n",
    "      #  COUNT_IT = 0\n",
    "      #  INFO_MEAN = 0\n",
    " #       if done:\n",
    "  #          episode_durations.append(t + 1)\n",
    "   #         plot_durations()\n",
    "    #        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a6ca6-8b53-480c-8784-b1b53521fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = f\"{HOMEDIR}/graph_tracer/traced_graph_8_layers.json\"\n",
    "# num_episodes = 100\n",
    "# for_gen = GraphRunner(file, ncpus, ncuda, num_episodes+((num_episodes + 19)//20) * 5, True, verbose=verbose, home_dir=HOMEDIR)\n",
    "# for_gen.run()\n",
    "# _ = for_gen.skip_initialization_routine()\n",
    "\n",
    "# BEST_TIME = 0\n",
    "# for i_episode in range(num_episodes):\n",
    "#     if (i_episode % 20 == 0):\n",
    "#         BEST_TIME = get_etalon(file, for_gen)\n",
    "#     # Initialize the environment and get its state\n",
    "#     my_slice = graph_slice(for_gen)\n",
    "#     agent.schedule(my_slice)\n",
    "#     cur_t = my_slice.get_timings()\n",
    "#     if (cur_t / BEST_TIME < best_res):\n",
    "#         best_res = cur_t /BEST_TIME\n",
    "#         best_model = agent\n",
    "#     for el in agent.frc:\n",
    "#         reward = torch.tensor(my_slice.get_time(el[3]) / (BEST_TIME * BEST_TIME), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "#         agent.memory.push(el[0], el[1], el[2], reward)\n",
    "#     trainer.step(agent = agent, timing = cur_t)\n",
    "#     #if (COUNT_IT == INFO_SZ):\n",
    "#       #  print(\"good_st = \", good_steps)\n",
    "#       #  print(\"normal metric = \", get_timings_policy(5, for_gen) / get_timings_for_acc(5, for_gen))\n",
    "#       #  COUNT_IT = 0\n",
    "#       #  INFO_MEAN = 0\n",
    "#  #       if done:\n",
    "#   #          episode_durations.append(t + 1)\n",
    "#    #         plot_durations()\n",
    "#     #        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0123f7-ae6a-48ad-bf09-fe7a7c48e54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = f\"{HOMEDIR}/graph_tracer/traced_graph_16_layers.json\"\n",
    "# num_episodes = 60\n",
    "# for_gen = GraphRunner(file, ncpus, ncuda, num_episodes+((num_episodes + 19)//20) * 5, True, verbose=verbose, home_dir=HOMEDIR)\n",
    "# for_gen.run()\n",
    "# _ = for_gen.skip_initialization_routine()\n",
    "# for i_episode in range(num_episodes):\n",
    "#     if (i_episode % 20 == 0):\n",
    "#         BEST_TIME = get_etalon(file, for_gen)\n",
    "#     # Initialize the environment and get its state\n",
    "#     my_slice = graph_slice(for_gen)\n",
    "#     agent.schedule(my_slice)\n",
    "#     cur_t = my_slice.get_timings()\n",
    "#     if (cur_t / BEST_TIME < best_res):\n",
    "#         best_res = cur_t /BEST_TIME\n",
    "#         best_model = agent\n",
    "#     for el in agent.frc:\n",
    "#         reward = torch.tensor(my_slice.get_time(el[3]) / (BEST_TIME * BEST_TIME), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "#         agent.memory.push(el[0], el[1], el[2], reward)\n",
    "#     trainer.step(agent = agent, timing = cur_t)\n",
    "#     #if (COUNT_IT == INFO_SZ):\n",
    "#       #  print(\"good_st = \", good_steps)\n",
    "#       #  print(\"normal metric = \", get_timings_policy(5, for_gen) / get_timings_for_acc(5, for_gen))\n",
    "#       #  COUNT_IT = 0\n",
    "#       #  INFO_MEAN = 0\n",
    "#  #       if done:\n",
    "#   #          episode_durations.append(t + 1)\n",
    "#    #         plot_durations()\n",
    "#     #        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccbbbc4-0fd1-4a04-973c-bd275978ccbc",
   "metadata": {},
   "source": [
    "# Look, difference is 1.5%\n",
    "\n",
    "It is really small, be carefull   \n",
    "Just because testing graph setup is really small. Graph bigger, and if cant store in gpu - you will start feeling\n",
    "\n",
    "For now we are not explored yet:\n",
    "1. Tensor/Pipeline parallelism\n",
    "2. Heterogenous setup (different gpus)\n",
    "3. ofloading\n",
    "4. backward passes and gradient chekpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f63805-c4ce-47b1-80e4-004b7398641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc0018a-45f7-4157-b36a-da98afed68ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent2 = BaselineAgent(strategy=\"dynamic\", device=\"cpu\" if ncuda==0 else \"cuda\")\n",
    "trainer = AgentTrainer(agent)\n",
    "gr = GraphRunner(file, ncpus, ncuda, 2 * 20, manual_sampling, verbose=verbose, home_dir=HOMEDIR)\n",
    "gr.run(interactive=True, starpu_home=STARPU_HOME)\n",
    "_ = gr.skip_initialization_routine()\n",
    "timings_dynamic = []\n",
    "\n",
    "for i in range(gr.niters // 2):\n",
    "    # inds = get_subset_inds(rng, total_operations)\n",
    "    inds = \"all\" # just use this if you want whole graph\n",
    "    print(f\"{i=} {inds=}\")\n",
    "    m = graph_slice(gr)\n",
    "    best_model.schedule(m)\n",
    "    timings = m.get_timings()\n",
    "    graph = gr.next(inds=inds)\n",
    "\n",
    "    # print(\"Nodes: \", len(graph.nodes))\n",
    "\n",
    "    agent2.schedule(graph)\n",
    "\n",
    "    gr.serialize_priorities(graph)\n",
    "    timings2 = gr.wait_get_timings()\n",
    "    \n",
    "    timings_dynamic.append(timings / timings2)\n",
    "print(timings_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f48c5c-e4cf-4c5a-9067-d967e2b960bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = [i for i in range(len(epi_to_ac))]\n",
    "plt.plot(x, epi_to_ac)\n",
    "plt.show()\n",
    "plt.plot(x, epi_to_score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0554e-0f11-474b-9dc2-49d4cb883390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16f3eca7-eeeb-4c63-9863-6fb888a64d09",
   "metadata": {},
   "source": [
    "## Caveats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bacc3ea-d07e-4c97-80de-2568d5c18d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if metric collection done correctly - on every run you will have filled length_mcs\n",
    "# it is NOT UPDATING in real time\n",
    "# it is just statistics collected on collect_metrics=True and not updated since\n",
    "# they are stored in STARPU_HOME/.starpu/sampling/codelets/*some_number*/\n",
    "graph.nodes[list(graph.nodes.keys())[50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38716921-8599-429a-ba93-2b746acd942f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
